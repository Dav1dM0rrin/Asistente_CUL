# Archivo: chatbot/llm_service.py
# Este servicio encapsula la l贸gica para interactuar con el modelo de lenguaje Gemini.
# MODIFICADO PARA EL CHATBOT UNIVERSITARIO CUL

import google.generativeai as genai
from chatbot.config import GEMINI_API_KEY # Importa la clave API desde la configuraci贸n.
from chatbot.bot_logging import logger # Usa el logger configurado para la aplicaci贸n.
import json # Necesario para parsear respuestas JSON del LLM y formatear datos.

# --- Constantes y Configuraci贸n del Modelo ---
MODEL_NAME = 'gemini-1.5-flash-latest' # Modelo de Gemini a utilizar.
USER_ROLE = "user" # Rol del usuario en el historial de chat.
MODEL_ROLE = "model" # Rol del modelo (Gemini) en el historial de chat.

# Configuraci贸n global del SDK de Gemini.
try:
    genai.configure(api_key=GEMINI_API_KEY)
    generation_config = genai.types.GenerationConfig(
        # temperature=0.7, # Podr铆as ajustar la creatividad.
        # max_output_tokens=2048, # Ajustar seg煤n necesidad.
    )
    # safety_settings = [ ... ] # Ajustar filtros de contenido si es necesario.

    llm_model = genai.GenerativeModel(
        model_name=MODEL_NAME,
        generation_config=generation_config,
    )
    logger.info(f"Modelo Gemini ({MODEL_NAME}) configurado exitosamente para Chatbot CUL.")
except Exception as e:
    logger.critical(f"CRTICO: Error al configurar el SDK de Gemini: {e}. El servicio LLM no funcionar谩.", exc_info=True)
    llm_model = None

conversation_sessions_store = {}

# --- Prompt de Sistema Detallado para el Chatbot Universitario CUL ---
SYSTEM_PROMPT_TEXT = """
Eres 'Asistente CUL', un chatbot IA avanzado, amigable, emp谩tico y muy servicial, desarrollado para la Corporaci贸n Universitaria Latinoamericana (CUL). Tu objetivo principal es asistir a estudiantes, aspirantes y personal con sus consultas sobre la vida universitaria.

Tus Prop贸sitos Principales son:
1.  **Informaci贸n Acad茅mica**: Responder preguntas sobre:
    * Tr谩mites acad茅micos (inscripciones, matr铆culas, certificados, homologaciones, etc.).
    * Horarios de clases, asignaturas, y disponibilidad de aulas (si tienes acceso a esta informaci贸n).
    * Calendario acad茅mico (fechas importantes, periodos de ex谩menes, vacaciones).
    * Programas ofrecidos (pregrados, posgrados, cursos de extensi贸n).
    * Requisitos de admisi贸n y proceso de inscripci贸n.
2.  **Soporte T茅cnico B谩sico**: Ofrecer gu铆a para problemas t茅cnicos comunes como:
    * Acceso a plataformas virtuales (Moodle, sistema de notas).
    * Problemas con el correo institucional.
    * Conexi贸n a la red Wi-Fi de la universidad.
    * Uso b谩sico de software licenciado por la universidad.
3.  **Informaci贸n General de la Universidad**:
    * Ubicaci贸n de campus, facultades, oficinas administrativas, biblioteca, cafeter铆as.
    * Servicios al estudiante (bienestar universitario, deportes, cultura, orientaci贸n psicol贸gica).
    * Eventos universitarios.
    * Informaci贸n de contacto de diferentes dependencias.
4.  **Generaci贸n de Tickets para Asistencia Humana**:
    * Si una consulta es demasiado compleja, requiere informaci贸n personal sensible que no puedes manejar, o el usuario solicita expl铆citamente "hablar con un humano", "necesito ayuda de una persona", "generar un ticket", o frases similares, debes ofrecer generar un ticket para que un miembro del personal de la CUL se ponga en contacto.
    * Para generar un ticket, necesitar谩s una breve descripci贸n del problema o consulta del usuario.

Capacidades Clave y Comportamiento Esperado:
-   **Lenguaje y Tono**: Usa un espa帽ol colombiano claro, amigable, formal pero cercano, y respetuoso. S茅 paciente y comprensivo.
-   **Clarificaci贸n**: Si la pregunta del usuario es ambigua, pide amablemente que la reformule o d茅 m谩s detalles. No asumas.
-   **Veracidad**: 隆No inventes informaci贸n! Si no tienes la informaci贸n certera, es mejor decir "No tengo esa informaci贸n en este momento, pero te sugiero consultar la p谩gina web oficial de la CUL ([enlacedelapaginaweb.cul.edu.co] si lo tienes) o contactar directamente con la oficina correspondiente." o "No encontr茅 datos sobre eso."
-   **Concisi贸n y Formato**: Mant茅n las respuestas razonablemente concisas y bien formateadas para Telegram.
-   **Contexto Local**: Recuerda que representas a la Corporaci贸n Universitaria Latinoamericana (CUL).
-   **Limitaciones**: Eres un IA. No puedes realizar acciones que requieran acceso a datos personales o sistemas internos sin la debida autorizaci贸n o integraci贸n. Tu funci贸n es informar y guiar.
-   **Uso de Datos Externos (RAG)**: Si se te proporciona 'Contexto de la base de datos de FAQs' en un mensaje, 煤salo para responder la consulta actual. No lo memorices para futuras preguntas no relacionadas.
-   **Comandos**: Recuerda al usuario comandos 煤tiles como /ayuda, /contacto, /ticket, /reset_chat cuando sea apropiado.

Ejemplo de interacci贸n para generar ticket:
Usuario: "No puedo ingresar a la plataforma de notas y ya intent茅 recuperar mi contrase帽a."
T煤: "Entiendo. Lamento que est茅s teniendo problemas para acceder a la plataforma de notas. Como ya intentaste recuperar tu contrase帽a sin 茅xito, puedo ayudarte a generar un ticket para que nuestro equipo de soporte t茅cnico revise tu caso. 驴Te gustar铆a que lo hagamos?"
Usuario: "S铆, por favor."
T煤: "Perfecto. Por favor, dame una breve descripci贸n del problema, incluyendo tu nombre completo y n煤mero de identificaci贸n estudiantil (si lo tienes a mano) para incluirlo en el ticket."
"""

async def generate_response(user_id: str, user_message: str, api_data_context: dict = None) -> str:
    """
    Genera una respuesta utilizando el LLM, manteniendo un historial de conversaci贸n por usuario.
    """
    if not llm_model:
        logger.error(f"LLM_SERVICE_CUL: Modelo Gemini no disponible para user_id {user_id}.")
        return "Lo siento, estoy experimentando dificultades t茅cnicas con mi asistente inteligente en este momento. Por favor, intenta de nuevo m谩s tarde."

    if user_id not in conversation_sessions_store:
        initial_history = [
            {"role": USER_ROLE, "parts": [{"text": SYSTEM_PROMPT_TEXT}]},
            {"role": MODEL_ROLE, "parts": [{"text": "隆Hola! Soy Asistente CUL. Estoy aqu铆 para ayudarte con tus consultas sobre la Corporaci贸n Universitaria Latinoamericana. 驴En qu茅 puedo colaborarte hoy? "}]}
        ]
        conversation_sessions_store[user_id] = llm_model.start_chat(history=initial_history)
        logger.info(f"LLM_SERVICE_CUL: Nueva sesi贸n de chat iniciada para user_id {user_id}.")
    
    chat_session = conversation_sessions_store[user_id]

    message_parts_for_llm = [{"text": user_message}]
    if api_data_context:
        context_info_text = (
            "\n\n--- Contexto de la base de datos de FAQs (para tu informaci贸n interna al responder la pregunta actual del usuario) ---\n"
            f"{json.dumps(api_data_context, indent=2, ensure_ascii=False)}\n"
            "--- Fin del contexto ---"
        )
        message_parts_for_llm.append({"text": context_info_text})
        logger.debug(f"LLM_SERVICE_CUL: A帽adiendo contexto de API/FAQs al mensaje para user_id {user_id}: {context_info_text[:200]}...")

    try:
        logger.debug(f"LLM_SERVICE_CUL: Enviando mensaje a Gemini para user_id {user_id}. Mensaje (inicio): {str(message_parts_for_llm)[:300]}...")
        
        llm_api_response = await chat_session.send_message_async(message_parts_for_llm)
        
        if llm_api_response.prompt_feedback and llm_api_response.prompt_feedback.block_reason:
            block_reason = llm_api_response.prompt_feedback.block_reason
            block_message = f"Respuesta bloqueada por pol铆tica de seguridad: {block_reason}."
            logger.warning(f"LLM_SERVICE_CUL: Respuesta de Gemini bloqueada para user_id {user_id}. Raz贸n: {block_reason}. Feedback: {llm_api_response.prompt_feedback}")
            return f"No pude generar una respuesta completa debido a una restricci贸n de contenido ({block_reason}). Por favor, reformula tu pregunta."

        generated_text = llm_api_response.text
        logger.info(f"LLM_SERVICE_CUL: Respuesta recibida de Gemini para user_id {user_id}: {generated_text[:300]}...")
        return generated_text

    except Exception as e:
        logger.error(f"LLM_SERVICE_CUL: Error al generar respuesta con Gemini para user_id {user_id}: {e}", exc_info=True)
        if hasattr(e, 'response') and hasattr(e.response, 'prompt_feedback'):
            logger.error(f"LLM_SERVICE_CUL: Prompt Feedback de Gemini en error: {e.response.prompt_feedback}")
        return "Lo siento, tuve un problema t茅cnico al intentar procesar tu solicitud con el asistente inteligente. Por favor, intenta de nuevo en unos momentos."


async def classify_intent_and_extract_entities(user_message: str) -> dict:
    """
    Utiliza el LLM para clasificar la intenci贸n y extraer entidades relevantes para el contexto universitario.
    """
    if not llm_model:
        logger.error("LLM_SERVICE_CUL: Modelo Gemini no disponible para clasificaci贸n de intenci贸n.")
        return {"error": "Modelo Gemini no disponible."}

    classification_prompt_text = f"""
    Analiza el siguiente mensaje de usuario en el contexto de la Corporaci贸n Universitaria Latinoamericana (CUL) y determina su intenci贸n principal y extrae entidades relevantes.
    Intenciones posibles:
    - CONSULTA_TRAMITE_ACADEMICO (ej: "驴c贸mo me inscribo?", "costo de la matr铆cula", "驴qu茅 necesito para un certificado?")
    - CONSULTA_HORARIO (ej: "驴cu谩ndo tengo clase de c谩lculo?", "horario de la biblioteca")
    - CONSULTA_PROGRAMA_ACADEMICO (ej: "informaci贸n sobre ingenier铆a de sistemas", "驴tienen posgrados en derecho?")
    - SOLICITUD_SOPORTE_TECNICO (ej: "no puedo entrar a Moodle", "problemas con el wifi")
    - INFORMACION_GENERAL_CUL (ej: "驴d贸nde queda la cafeter铆a?", "tel茅fono de admisiones")
    - GENERAR_TICKET_HUMANO (ej: "quiero hablar con una persona", "necesito ayuda de un asesor", "generar un ticket")
    - SALUDO
    - DESPEDIDA
    - AFIRMACION
    - NEGACION
    - CANCELAR_ACCION
    - PREGUNTA_GENERAL_CHATBOT (ej: "驴qui茅n eres?", "驴qu茅 puedes hacer?")
    - DESCONOCIDO

    Para CONSULTA_TRAMITE_ACADEMICO, extrae si es posible: "nombre_tramite" (string), "documento_relacionado" (string).
    Para CONSULTA_HORARIO, extrae si es posible: "nombre_asignatura" (string), "dia_semana" (string), "lugar" (string, ej: "biblioteca", "laboratorio X").
    Para CONSULTA_PROGRAMA_ACADEMICO, extrae: "nombre_programa" (string), "nivel_academico" (string, ej: "pregrado", "posgrado", "diplomado").
    Para SOLICITUD_SOPORTE_TECNICO, extrae: "descripcion_problema" (string), "plataforma_afectada" (string, ej: "Moodle", "correo", "wifi").
    Para GENERAR_TICKET_HUMANO, extrae: "resumen_solicitud_ticket" (string, un breve resumen de por qu茅 necesita el ticket).
    Para INFORMACION_GENERAL_CUL, extrae: "tema_consulta_general" (string, ej: "ubicaci贸n", "contacto", "servicio").

    Mensaje del usuario: "{user_message}"

    Responde NICAMENTE en formato JSON v谩lido con las claves "intent" (string) y "entities" (objeto JSON).
    Si no se extraen entidades, "entities" debe ser un objeto vac铆o {{}}.
    Si la intenci贸n no es clara o no encaja en las categor铆as, usa "DESCONOCIDO".

    Ejemplos de respuesta JSON esperada:
    Si el usuario dice "驴cu谩les son los requisitos para la inscripci贸n a psicolog铆a?":
    {{
      "intent": "CONSULTA_TRAMITE_ACADEMICO",
      "entities": {{
        "nombre_tramite": "inscripci贸n",
        "documento_relacionado": "requisitos",
        "nombre_programa": "psicolog铆a" 
      }}
    }}

    Si el usuario dice "Necesito ayuda, no me funciona el correo institucional":
    {{
      "intent": "SOLICITUD_SOPORTE_TECNICO",
      "entities": {{
        "descripcion_problema": "no funciona el correo institucional",
        "plataforma_afectada": "correo institucional"
      }}
    }}
    
    Si el usuario dice "Quiero hablar con alguien de admisiones":
    {{
      "intent": "GENERAR_TICKET_HUMANO",
      "entities": {{
        "resumen_solicitud_ticket": "hablar con alguien de admisiones"
      }}
    }}
    """
    try:
        logger.debug(f"LLM_SERVICE_CUL: Enviando prompt de clasificaci贸n/extracci贸n a Gemini (inicio): {classification_prompt_text[:300]}...")
        
        llm_classification_response = await llm_model.generate_content_async(classification_prompt_text)
        
        if llm_classification_response.prompt_feedback and llm_classification_response.prompt_feedback.block_reason:
            block_reason = llm_classification_response.prompt_feedback.block_reason
            logger.warning(f"LLM_SERVICE_CUL: Respuesta de clasificaci贸n de Gemini bloqueada. Raz贸n: {block_reason}.")
            return {"intent": "DESCONOCIDO", "entities": {}, "error": f"Respuesta de clasificaci贸n bloqueada ({block_reason})", "raw_response": ""}

        raw_json_text = llm_classification_response.text
        logger.info(f"LLM_SERVICE_CUL: Respuesta de clasificaci贸n (raw JSON) de Gemini: {raw_json_text}")

        clean_json_text = raw_json_text.strip()
        if clean_json_text.startswith("```json"):
            clean_json_text = clean_json_text[7:-3].strip()
        elif clean_json_text.startswith("```"):
             clean_json_text = clean_json_text[3:-3].strip()
        
        parsed_response = json.loads(clean_json_text)
        
        if not isinstance(parsed_response, dict) or \
           "intent" not in parsed_response or \
           not isinstance(parsed_response.get("intent"), str) or \
           "entities" not in parsed_response or \
           not isinstance(parsed_response.get("entities"), dict):
            raise ValueError("El JSON de respuesta del LLM para clasificaci贸n no tiene la estructura esperada (intent/entities).")
            
        logger.info(f"LLM_SERVICE_CUL: Intenci贸n y entidades extra铆das: {parsed_response}")
        return parsed_response

    except json.JSONDecodeError as json_err:
        logger.error(f"LLM_SERVICE_CUL: Error al parsear JSON de Gemini para clasificaci贸n: {json_err}. Respuesta original: {raw_json_text}", exc_info=True)
        return {"intent": "DESCONOCIDO", "entities": {}, "error": "Error al parsear la estructura de la respuesta del LLM.", "raw_response": raw_json_text}
    except Exception as e:
        logger.error(f"LLM_SERVICE_CUL: Error en clasificaci贸n de intenci贸n con Gemini: {e}", exc_info=True)
        if hasattr(e, 'response') and hasattr(e.response, 'prompt_feedback'):
            logger.error(f"LLM_SERVICE_CUL: Prompt Feedback de Gemini (clasificaci贸n) en error: {e.response.prompt_feedback}")
        return {"intent": "DESCONOCIDO", "entities": {}, "error": f"Error inesperado durante la clasificaci贸n: {type(e).__name__}"}


async def reset_conversation_history(user_id: str) -> bool:
    """
    Limpia/resetea el historial de conversaci贸n para un usuario espec铆fico.
    """
    if user_id in conversation_sessions_store:
        del conversation_sessions_store[user_id]
        logger.info(f"LLM_SERVICE_CUL: Historial de conversaci贸n reseteado para el usuario {user_id}.")
        return True
    logger.info(f"LLM_SERVICE_CUL: No se encontr贸 historial para resetear para el usuario {user_id}.")
    return False
